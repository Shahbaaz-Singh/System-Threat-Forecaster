{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-31T04:29:26.379186Z",
     "iopub.status.busy": "2025-03-31T04:29:26.378921Z",
     "iopub.status.idle": "2025-03-31T04:29:26.645693Z",
     "shell.execute_reply": "2025-03-31T04:29:26.644864Z",
     "shell.execute_reply.started": "2025-03-31T04:29:26.379168Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T04:29:28.649092Z",
     "iopub.status.busy": "2025-03-31T04:29:28.648679Z",
     "iopub.status.idle": "2025-03-31T04:29:30.044322Z",
     "shell.execute_reply": "2025-03-31T04:29:30.043615Z",
     "shell.execute_reply.started": "2025-03-31T04:29:28.649067Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Loading the dataset to a dataframe named 'data'\n",
    "data = pd.read_csv('train.csv')\n",
    "X = data.drop(columns=['target'])\n",
    "y = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Milestone 1\n",
    "\n",
    "# Exploratory Data Analysis (EDA)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Dataset Info:\")\n",
    "data.info()\n",
    "\n",
    "print(\"\\nFirst Few Rows:\")\n",
    "print(data.head())\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(data.describe())\n",
    "\n",
    "num_columns = len(data.columns)\n",
    "\n",
    "column_names = list(data.columns)\n",
    "\n",
    "num_rows = data.shape[0]\n",
    "\n",
    "print(f\"Number of columns: {num_columns}\")\n",
    "print(f\"Column names: {column_names}\")\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "\n",
    "if 'TotalPhysicalRAMMB' in data.columns:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(data['TotalPhysicalRAMMB'], kde=True, bins=30, color='blue')\n",
    "    plt.title('Distribution of Total Physical RAM (MB)')\n",
    "    plt.xlabel('Total Physical RAM (MB)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "if 'IsGamer' in data.columns:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.countplot(x='IsGamer', data=data, palette='viridis')\n",
    "    plt.title('Count of Gamers vs Non-Gamers')\n",
    "    plt.xlabel('Is Gamer')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "if 'PrimaryDisplayResolutionHorizontal' in data.columns and 'PrimaryDisplayResolutionVertical' in data.columns:\n",
    "    resolution_count = data[\n",
    "        (data['PrimaryDisplayResolutionHorizontal'] == 1366) & \n",
    "        (data['PrimaryDisplayResolutionVertical'] == 768)\n",
    "    ].shape[0]\n",
    "    print(f\"\\nNumber of systems with resolution 1366 x 768: {resolution_count}\")\n",
    "\n",
    "if 'NumAntivirusProductsInstalled' in data.columns:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(y='NumAntivirusProductsInstalled', data=data, palette='Set2')\n",
    "    plt.title('Boxplot of NumAntivirusProductsInstalled')\n",
    "    plt.ylabel('Number of Antivirus Products Installed')\n",
    "    plt.show()\n",
    "\n",
    "if 'IsPassiveModeEnabled' in data.columns and 'RealTimeProtectionState' in data.columns:\n",
    "    passive_mode_data = data[data['IsPassiveModeEnabled'] == 1]\n",
    "    most_frequent_rtp_state = passive_mode_data['RealTimeProtectionState'].mode()[0]\n",
    "    print(f\"\\nMost frequent value of RealTimeProtectionState when IsPassiveModeEnabled = 1: {most_frequent_rtp_state}\")\n",
    "\n",
    "high_cardinality_columns = [col for col in data.columns if data[col].nunique() > 50]\n",
    "filtered_data = data.drop(columns=high_cardinality_columns)\n",
    "\n",
    "sampled_data = filtered_data.sample(n=min(10000, len(filtered_data)), random_state=42)\n",
    "\n",
    "print(f\"Excluded high cardinality columns: {high_cardinality_columns}\")\n",
    "\n",
    "numerical_columns = sampled_data.select_dtypes(include=['int64', 'float64']).columns\n",
    "for column in numerical_columns:\n",
    "    if sampled_data[column].nunique() > 1:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.histplot(sampled_data[column], kde=True, bins=30, color='blue')\n",
    "        plt.title(f'Distribution of {column}')\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n",
    "\n",
    "categorical_columns = sampled_data.select_dtypes(include=['object']).columns\n",
    "for column in categorical_columns:\n",
    "    if sampled_data[column].nunique() <= 10:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.countplot(y=column, data=sampled_data, order=sampled_data[column].value_counts().index, palette='viridis')\n",
    "        plt.title(f'Distribution of {column}')\n",
    "        plt.xlabel('Count')\n",
    "        plt.ylabel(column)\n",
    "        plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = sampled_data.select_dtypes(include=['int64', 'float64']).corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T04:29:53.458305Z",
     "iopub.status.busy": "2025-03-31T04:29:53.457908Z",
     "iopub.status.idle": "2025-03-31T04:29:55.076672Z",
     "shell.execute_reply": "2025-03-31T04:29:55.075727Z",
     "shell.execute_reply.started": "2025-03-31T04:29:53.458281Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Milestone 2\n",
    "\n",
    "df_copy =data.copy()\n",
    "\n",
    "columns_to_check = ['IsBetaUser', 'IsPassiveModeEnabled', 'AntivirusConfigID', 'AutoSampleSubmissionEnabled', 'IsFlightsDisabled']\n",
    "\n",
    "redundant_columns = [col for col in columns_to_check if df_copy[col].nunique() == 1]\n",
    "print(\"Redundant Columns:\", redundant_columns)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "categorical_cols = df_copy.select_dtypes(include=['object']).columns\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    df_copy[col] = encoder.fit_transform(df_copy[col])\n",
    "corr_matrix = df_copy.corr()\n",
    "corr_pairs = corr_matrix.unstack()\n",
    "corr_pairs =corr_pairs[corr_pairs < 1].sort_values(ascending=False)\n",
    "print(corr_pairs.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# After we analysed the redundant features\n",
    "X = X.drop(columns=['IsBetaUser', 'AutoSampleSubmissionEnabled', 'IsFlightsDisabled'])\n",
    "\n",
    "# After we analysed the covariance between the features\n",
    "X = X.drop(columns=['OSSkuFriendlyName','PlatformType','OSArchitecture','OSBuildNumberOnly','NumericOSVersion','OSInstallLanguageID', 'OSProductSuite'])\n",
    "\n",
    "# MachineID don't have any significant contribution to the models\n",
    "X = X.drop(columns=['MachineID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing:\n",
    "\n",
    "# Include Imputation\n",
    "# Include Encoding\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# We will use different encoding for different cardinality features so that dimensionality is under control\n",
    "\n",
    "low_cardinality_features = [col for col in categorical_features if X[col].nunique() <= 10]\n",
    "high_cardinality_features = [col for col in categorical_features if X[col].nunique() > 10]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', IterativeImputer(max_iter=20, random_state=42))\n",
    "])\n",
    "\n",
    "low_card_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "high_card_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('low_card', low_card_transformer, low_cardinality_features),\n",
    "        ('high_card', high_card_transformer, high_cardinality_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_imputed_encoded = preprocessor.fit_transform(X)\n",
    "\n",
    "numeric_feature_names = numeric_features.tolist()\n",
    "low_card_feature_names = preprocessor.named_transformers_['low_card']['onehot'].get_feature_names_out(low_cardinality_features).tolist()\n",
    "high_card_feature_names = high_cardinality_features\n",
    "\n",
    "all_feature_names = numeric_feature_names + low_card_feature_names + high_card_feature_names\n",
    "\n",
    "X_imputed_encoded_df = pd.DataFrame(X_imputed_encoded, columns=all_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X1 = X_imputed_encoded_df\n",
    "y1 = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Final Model\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X1, y1, test_size=0.2, random_state=42, stratify=y1)\n",
    "\n",
    "param_dist = {\n",
    "    'randomforestclassifier__n_estimators': [50, 100, 200, 300],\n",
    "    'randomforestclassifier__max_depth': [None, 10, 20, 30],\n",
    "    'randomforestclassifier__min_samples_split': [2, 5, 10],\n",
    "    'randomforestclassifier__min_samples_leaf': [1, 2, 4],\n",
    "    'randomforestclassifier__bootstrap': [True, False],\n",
    "    'randomforestclassifier__class_weight': ['balanced', None]\n",
    "}\n",
    "\n",
    "pipeline = make_pipeline(StandardScaler(), RandomForestClassifier(random_state=42))\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,\n",
    "    cv=10,\n",
    "    scoring='f1',\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_params_random = random_search.best_params_\n",
    "best_model_random = random_search.best_estimator_\n",
    "\n",
    "print(\"Best Parameters from RandomizedSearchCV:\", best_params_random)\n",
    "\n",
    "param_grid = {\n",
    "    'randomforestclassifier__n_estimators': [\n",
    "        max(10, best_params_random['randomforestclassifier__n_estimators'] - 50),\n",
    "        best_params_random['randomforestclassifier__n_estimators'],\n",
    "        best_params_random['randomforestclassifier__n_estimators'] + 50],\n",
    "    'randomforestclassifier__max_depth': [\n",
    "        None if best_params_random['randomforestclassifier__max_depth'] is None else max(1, best_params_random['randomforestclassifier__max_depth'] - 5),\n",
    "        best_params_random['randomforestclassifier__max_depth'],\n",
    "        None if best_params_random['randomforestclassifier__max_depth'] is None else best_params_random['randomforestclassifier__max_depth'] + 5],\n",
    "    'randomforestclassifier__min_samples_split': [best_params_random['randomforestclassifier__min_samples_split']],\n",
    "    'randomforestclassifier__min_samples_leaf': [best_params_random['randomforestclassifier__min_samples_leaf']],\n",
    "    'randomforestclassifier__bootstrap': [best_params_random['randomforestclassifier__bootstrap']],\n",
    "    'randomforestclassifier__class_weight': [best_params_random.get('randomforestclassifier__class_weight', None)]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=10,\n",
    "    scoring='f1',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params_grid = grid_search.best_params_\n",
    "best_model_grid = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best Parameters from GridSearchCV:\", best_params_grid)\n",
    "\n",
    "y_pred = best_model_grid.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "report = classification_report(y_val, y_pred)\n",
    "\n",
    "print(f\"Validation Accuracy after GridSearchCV: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "print(\"\\Confusion Matrix:\\n\", confusion_matrix(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# SGD Model\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "param_dist = {\n",
    "    'sgdclassifier__alpha': [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3],\n",
    "    'sgdclassifier__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'sgdclassifier__max_iter': [1000, 3000, 5000, 10000, 20000],\n",
    "    'sgdclassifier__learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "    'sgdclassifier__eta0': [0.0001, 0.001, 0.01],\n",
    "    'sgdclassifier__early_stopping': [True],\n",
    "    'sgdclassifier__validation_fraction': [0.1, 0.2],\n",
    "    'sgdclassifier__class_weight': ['balanced', None],\n",
    "    'sgdclassifier__tol': [1e-4, 1e-3, 1e-2]\n",
    "}\n",
    "\n",
    "pipeline = make_pipeline(StandardScaler(), SGDClassifier(loss='log_loss', random_state=42))\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,\n",
    "    cv=10,\n",
    "    scoring='f1',\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_params_random = random_search.best_params_\n",
    "best_model_random = random_search.best_estimator_\n",
    "\n",
    "print(\"Best Parameters from RandomizedSearchCV:\", best_params_random)\n",
    "\n",
    "# Use the best parameters from RandomizedSearchCV to define a narrower grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'sgdclassifier__alpha': [best_params_random['sgdclassifier__alpha'] * 0.1,\n",
    "                             best_params_random['sgdclassifier__alpha'],\n",
    "                             best_params_random['sgdclassifier__alpha'] * 10],\n",
    "    'sgdclassifier__penalty': [best_params_random['sgdclassifier__penalty']],\n",
    "    'sgdclassifier__max_iter': [best_params_random['sgdclassifier__max_iter']],\n",
    "    'sgdclassifier__learning_rate': [best_params_random['sgdclassifier__learning_rate']],\n",
    "    'sgdclassifier__eta0': [best_params_random['sgdclassifier__eta0']],\n",
    "    'sgdclassifier__early_stopping': [True],\n",
    "    'sgdclassifier__validation_fraction': [best_params_random['sgdclassifier__validation_fraction']],\n",
    "    'sgdclassifier__class_weight': [best_params_random.get('sgdclassifier__class_weight', None)]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=10,\n",
    "    scoring='f1',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params_grid = grid_search.best_params_\n",
    "best_model_grid_sgd = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best Parameters from GridSearchCV:\", best_params_grid)\n",
    "\n",
    "y_pred = best_model_grid_sgd.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "report = classification_report(y_val, y_pred)\n",
    "\n",
    "print(f\"Validation Accuracy after GridSearchCV: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "print(\"\\Confusion Matrix:\\n\", confusion_matrix(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T05:19:24.891733Z",
     "iopub.status.busy": "2025-03-31T05:19:24.891362Z",
     "iopub.status.idle": "2025-03-31T05:20:34.499492Z",
     "shell.execute_reply": "2025-03-31T05:20:34.498330Z",
     "shell.execute_reply.started": "2025-03-31T05:19:24.891706Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression Model\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "X_log = data.drop(columns=['target'])\n",
    "y_log = data['target']\n",
    "\n",
    "numeric_features = X_log.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = X_log.select_dtypes(include=['object']).columns\n",
    "\n",
    "numeric_imputer = SimpleImputer(strategy='median')\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "X_log[numeric_features] = numeric_imputer.fit_transform(X_log[numeric_features])\n",
    "X_log[categorical_features] = categorical_imputer.fit_transform(X_log[categorical_features])\n",
    "\n",
    "label_encoders = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    X_log[col] = le.fit_transform(X_log[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_log[numeric_features] = scaler.fit_transform(X_log[numeric_features])\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "numeric_poly = poly.fit_transform(X_log[numeric_features])\n",
    "poly_feature_names = [f'poly_{i}' for i in range(numeric_poly.shape[1])]\n",
    "X_poly = pd.DataFrame(numeric_poly, columns=poly_feature_names)\n",
    "\n",
    "X_log = pd.concat([X_log, X_poly], axis=1)\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=50)  # Select top 50 features\n",
    "X_selected = selector.fit_transform(X_log, y_log)\n",
    "selected_features_mask = selector.get_support()\n",
    "\n",
    "all_features = list(X_log.columns)\n",
    "selected_features = [feature for feature, selected in zip(all_features, selected_features_mask) if selected]\n",
    "\n",
    "X_log = X_log[selected_features]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_log, y_log, test_size=0.2, random_state=42, stratify=y_log)\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_log), y=y_log)\n",
    "class_weight_dict = dict(zip(np.unique(y_log), class_weights))\n",
    "\n",
    "lr_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight=class_weight_dict,\n",
    "    C=0.1,\n",
    "    solver='saga',\n",
    "    penalty='l1',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr_model.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "report = classification_report(y_val, y_pred)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "print(\"\\nConfusion Matrix:\\n\",confusion_matrix(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T05:29:43.838372Z",
     "iopub.status.busy": "2025-03-31T05:29:43.837411Z",
     "iopub.status.idle": "2025-03-31T05:29:43.867690Z",
     "shell.execute_reply": "2025-03-31T05:29:43.866230Z",
     "shell.execute_reply.started": "2025-03-31T05:29:43.838337Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Other Models\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score , classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X1, y1, test_size=0.2, random_state=42, stratify=y1)\n",
    "\n",
    "classifiers = [\n",
    "    ('GB', GradientBoostingClassifier(random_state=42)),\n",
    "    ('MLP', MLPClassifier(random_state=42)),\n",
    "]\n",
    "\n",
    "for name, clf in classifiers:\n",
    "    pipeline = make_pipeline(StandardScaler(), clf)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    score = pipeline.score(X_val, y_val)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "    print(f\"{name} Accuracy: {score:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred))\n",
    "    print(\"\\nConfusion Matrix:\\n\",confusion_matrix(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# I tried the gradient boosting model with hyperparameter tuning also but it was taking a very very long time so finally i decided to only use the Random forest as my final model\n",
    "\"\"\"\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X1, y1, test_size=0.2, random_state=42, stratify=y1)\n",
    "\n",
    "pipeline = make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state=42))\n",
    "\n",
    "param_dist = {\n",
    "    'gradientboostingclassifier__n_estimators': [50, 100, 200, 300],\n",
    "    'gradientboostingclassifier__max_depth': [3, 5, 7, 10],\n",
    "    'gradientboostingclassifier__min_samples_split': [2, 5, 10],\n",
    "    'gradientboostingclassifier__min_samples_leaf': [1, 2, 4],\n",
    "    'gradientboostingclassifier__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'gradientboostingclassifier__subsample': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    scoring='f1',\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "best_params_random = random_search.best_params_\n",
    "\n",
    "param_grid = {\n",
    "    'gradientboostingclassifier__n_estimators': [\n",
    "        max(10, best_params_random['gradientboostingclassifier__n_estimators'] - 50),\n",
    "        best_params_random['gradientboostingclassifier__n_estimators'],\n",
    "        best_params_random['gradientboostingclassifier__n_estimators'] + 50],\n",
    "    \n",
    "    'gradientboostingclassifier__max_depth': [\n",
    "        max(1, best_params_random['gradientboostingclassifier__max_depth'] - 2),\n",
    "        best_params_random['gradientboostingclassifier__max_depth'],\n",
    "        best_params_random['gradientboostingclassifier__max_depth'] + 2],\n",
    "\n",
    "    'gradientboostingclassifier__min_samples_split': [best_params_random['gradientboostingclassifier__min_samples_split']],\n",
    "    'gradientboostingclassifier__min_samples_leaf': [best_params_random['gradientboostingclassifier__min_samples_leaf']],\n",
    "    'gradientboostingclassifier__learning_rate': [best_params_random['gradientboostingclassifier__learning_rate']],\n",
    "    'gradientboostingclassifier__subsample': [best_params_random['gradientboostingclassifier__subsample']]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model_grid = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model_grid.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "report = classification_report(y_val, y_pred)\n",
    "\n",
    "print(f\"Validation Accuracy after GridSearchCV: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Droping unnecessary columns (same as training data preprocessing)\n",
    "test_data = test_data.drop(columns=['target','MachineID','OSSkuFriendlyName','PlatformType','OSArchitecture','OSBuildNumberOnly','NumericOSVersion','OSInstallLanguageID', 'OSProductSuite','IsBetaUser', 'AutoSampleSubmissionEnabled', 'IsFlightsDisabled'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Apply the same preprocessor to the test data\n",
    "X_test_imputed_encoded = preprocessor.transform(test_data)  # Use transform (not fit_transform) on test data\n",
    "\n",
    "# Convert the transformed test data back to a DataFrame with proper column names\n",
    "X_test_imputed_encoded_df = pd.DataFrame(X_test_imputed_encoded, columns=all_feature_names)\n",
    "\n",
    "# Make predictions using the trained model\n",
    "y_pred = best_model_grid.predict(X_test_imputed_encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Finding the accuracy of the final model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_test = pd.read_csv(\"true_labels.csv\")['target']\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on test set: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Milestone 1\n",
    "\n",
    "data_cleaned = data.dropna()\n",
    "\n",
    "unique_os_versions = data_cleaned['OSVersion'].nunique()\n",
    "print(f\"Number of unique OS versions: {unique_os_versions}\")\n",
    "max_antivirus_installed = data_cleaned['NumAntivirusProductsInstalled'].max()\n",
    "print(f\"Maximum value of NumAntivirusProductsInstalled: {max_antivirus_installed}\")\n",
    "gamers_with_malware = data_cleaned[(data_cleaned['IsGamer'] == 1) & (data_cleaned['target'] == 1)].shape[0]\n",
    "print(f\"Number of systems owned by gamers where malware was detected: {gamers_with_malware}\")\n",
    "most_frequent_rtp_state = data_cleaned[data_cleaned['IsPassiveModeEnabled'] == 1]['RealTimeProtectionState'].mode()[0]\n",
    "print(f\"Most frequent value of RealTimeProtectionState: {most_frequent_rtp_state}\")\n",
    "resolution_count = data_cleaned[\n",
    "    (data_cleaned['PrimaryDisplayResolutionHorizontal'] == 1366) & \n",
    "    (data_cleaned['PrimaryDisplayResolutionVertical'] == 768)\n",
    "].shape[0]\n",
    "\n",
    "print(f\"Number of systems with resolution 1366 x 768: {resolution_count}\")\n",
    "percentile_50_ram = data_cleaned['TotalPhysicalRAMMB'].quantile(0.5)\n",
    "print(f\"50th percentile value of TotalPhysicalRAMMB: {percentile_50_ram}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Milestone 2\n",
    "\n",
    "df = data.copy()\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_df = df.select_dtypes(include=['object'])\n",
    "low_cardinality_cols = cat_df.nunique()[cat_df.nunique() <= 10].index\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "encoded_cols = encoder.fit_transform(cat_df[low_cardinality_cols])\n",
    "\n",
    "encoded_df = pd.DataFrame(encoded_cols, columns=encoder.get_feature_names_out(low_cardinality_cols))\n",
    "\n",
    "cat_df = cat_df.drop(columns=low_cardinality_cols)\n",
    "cat_df = pd.concat([cat_df, encoded_df], axis=1)\n",
    "cat_df.shape[1]\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "num_df = data.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "num_df_scaled = pd.DataFrame(scaler.fit_transform(num_df), columns=num_df.columns)\n",
    "\n",
    "total_sum = num_df_scaled.sum().sum()\n",
    "\n",
    "print(\"The sum of all values in num_df after scaling is:\", total_sum)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "X_imputed[X.select_dtypes(include=['object']).columns] = encoder.fit_transform(\n",
    "    X_imputed[X.select_dtypes(include=['object']).columns]\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = SGDClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy Score:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Milestone 3\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = data.drop(columns=['target'])\n",
    "y = data['target']\n",
    "\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "numeric_cols = X.select_dtypes(include=['number']).columns\n",
    "\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "X[categorical_cols] = cat_imputer.fit_transform(X[categorical_cols])\n",
    "X[numeric_cols] = num_imputer.fit_transform(X[numeric_cols])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    X[col] = label_encoder.fit_transform(X[col])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "param_grid_dt = {\n",
    "    'max_depth': [20, 30],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "grid_search_dt = GridSearchCV(estimator=dt_model, param_grid=param_grid_dt, cv=3, scoring='accuracy')\n",
    "grid_search_dt.fit(X_train, y_train)\n",
    "\n",
    "best_params_dt = grid_search_dt.best_params_\n",
    "best_dt_model = grid_search_dt.best_estimator_\n",
    "val_accuracy_dt = accuracy_score(y_val, best_dt_model.predict(X_val))\n",
    "\n",
    "print(\"Best Parameters for Decision Tree:\", best_params_dt)\n",
    "print(\"Validation Accuracy (Decision Tree): {:.5f}\".format(val_accuracy_dt))\n",
    "print(round(val_accuracy_dt,2))\n",
    "\n",
    "adaboost_model = AdaBoostClassifier(random_state=42)\n",
    "param_grid_ada = {\n",
    "    'n_estimators': [10, 20, 30],\n",
    "    'learning_rate': [5, 10],\n",
    "    'algorithm': ['SAMME']\n",
    "}\n",
    "\n",
    "grid_search_ada = GridSearchCV(estimator=adaboost_model, param_grid=param_grid_ada, cv=3, scoring='accuracy')\n",
    "grid_search_ada.fit(X_train, y_train)\n",
    "\n",
    "best_params_ada = grid_search_ada.best_params_\n",
    "best_ada_model = grid_search_ada.best_estimator_\n",
    "val_accuracy_ada = accuracy_score(y_val, best_ada_model.predict(X_val))\n",
    "\n",
    "print(\"Best Parameters for AdaBoost:\", best_params_ada)\n",
    "print(\"Validation Accuracy (AdaBoost): {:.5f}\".format(val_accuracy_ada))\n",
    "print(round(val_accuracy_ada,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Milestone 4\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data_clean = data.dropna()\n",
    "\n",
    "X = data_clean.drop(columns=['target'])\n",
    "y = data_clean['target']\n",
    "\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "encoder = OrdinalEncoder()\n",
    "X[categorical_cols] = encoder.fit_transform(X[categorical_cols])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "components_70 = np.argmax(cumulative_variance >= 0.70) + 1\n",
    "print(f\"Number of components to explain 70% variance: {components_70}\")\n",
    "\n",
    "pca_70 = PCA(n_components=components_70, svd_solver='full')\n",
    "X_reduced_70 = pca_70.fit_transform(X_scaled)\n",
    "X_reconstructed = pca_70.inverse_transform(X_reduced_70)\n",
    "mse_70 = mean_squared_error(X_scaled, X_reconstructed)\n",
    "print(f\"MSE after reconstruction with {components_70} components: {mse_70}\")\n",
    "\n",
    "pca_40 = PCA(n_components=40, svd_solver='full')\n",
    "pca_40.fit(X_scaled)\n",
    "variance_40 = np.sum(pca_40.explained_variance_ratio_)\n",
    "print(f\"Variance explained by 40 components: {variance_40}\")\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=15)\n",
    "X_selected = selector.fit_transform(X_scaled, y)\n",
    "selected_indices = selector.get_support(indices=True)\n",
    "selected_features = X.columns[selected_indices]\n",
    "print(f\"Top 15 selected features: {list(selected_features)}\")\n",
    "\n",
    "to_check = [\n",
    "    'AntivirusConfigID',\n",
    "    'RegionIdentifier',\n",
    "    'OSBuildNumber',\n",
    "    'DateAS',\n",
    "    'SignatureVersion',\n",
    "    'PowerPlatformRole',\n",
    "    'IsPenCapable'\n",
    "]\n",
    "not_selected = [col for col in to_check if col not in selected_features]\n",
    "print(f\"Columns NOT in top 15 features: {not_selected}\")\n",
    "\n",
    "scores = selector.scores_[selected_indices]\n",
    "best_score_idx = np.argmax(scores)\n",
    "best_feature = selected_features[best_score_idx]\n",
    "best_score = scores[best_score_idx]\n",
    "print(f\"Best feature: {best_feature} with a score of {best_score}\")\n",
    "\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
    "\n",
    "k = 10\n",
    "selector = SelectKBest(score_func=f_regression, k=k)\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "selected_feature_indices = selector.get_support(indices=True)\n",
    "selected_features = [f\"Feature_{i}\" for i in selected_feature_indices]  # Placeholder names for selected features\n",
    "print(f\"Top {k} selected features: {selected_features}\")\n",
    "\n",
    "lasso = Lasso(alpha=1.0)  # Regularization strength (tune alpha as needed)\n",
    "lasso.fit(X_train_selected, y_train)\n",
    "lasso_coefficients = lasso.coef_\n",
    "\n",
    "print(\"\\nLasso Coefficients:\")\n",
    "for feature, coef in zip(selected_features, lasso_coefficients):\n",
    "    print(f\"{feature}: {coef}\")\n",
    "\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train_selected, y_train)\n",
    "ridge_coefficients = ridge.coef_\n",
    "\n",
    "print(\"\\nRidge Coefficients:\")\n",
    "for feature, coef in zip(selected_features, ridge_coefficients):\n",
    "    print(f\"{feature}: {coef}\")\n",
    "\n",
    "print(\"\\nInsights:\")\n",
    "print(\"Lasso tends to shrink some coefficients to exactly zero, effectively performing feature selection.\")\n",
    "print(\"Ridge shrinks coefficients but does not set them to zero, which can be useful when all features are informative.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T20:16:46.801469Z",
     "iopub.status.busy": "2025-03-11T20:16:46.801094Z",
     "iopub.status.idle": "2025-03-11T20:16:49.061104Z",
     "shell.execute_reply": "2025-03-11T20:16:49.060058Z",
     "shell.execute_reply.started": "2025-03-11T20:16:46.801440Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Milestone 5\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = data.copy()\n",
    "\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "X[categorical_cols] = cat_imputer.fit_transform(X[categorical_cols])\n",
    "X[numeric_cols] = num_imputer.fit_transform(X[numeric_cols])\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "X[categorical_cols] = encoder.fit_transform(X[categorical_cols])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model - 1\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_score\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "\n",
    "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
    "\n",
    "correct_classifications_dt = np.trace(cm_dt)\n",
    "incorrect_class1_as_0_dt = cm_dt[1, 0]\n",
    "precision_class0_dt = precision_score(y_test, y_pred_dt, pos_label=0)\n",
    "\n",
    "print(\"Decision Tree Results:\")\n",
    "print(f\"Correctly classified samples: {correct_classifications_dt}\")\n",
    "print(f\"Class 1 incorrectly classified as 0: {incorrect_class1_as_0_dt}\")\n",
    "print(f\"Precision for class 0: {precision_class0_dt:.3f}\")\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix, recall_score\n",
    "\n",
    "ada_model = AdaBoostClassifier(n_estimators=10, learning_rate=10, random_state=42)\n",
    "ada_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_ada = ada_model.predict(X_test)\n",
    "\n",
    "cm_ada = confusion_matrix(y_test, y_pred_ada)\n",
    "\n",
    "correct_classifications_ada = np.trace(cm_ada)\n",
    "incorrect_class1_as_0_ada = cm_ada[1, 0]\n",
    "recall_class1_ada = recall_score(y_test, y_pred_ada, pos_label=1)\n",
    "\n",
    "print(\"\\nAdaBoost Results:\")\n",
    "print(f\"Correctly classified samples: {correct_classifications_ada}\")\n",
    "print(f\"Class 1 incorrectly classified as 0: {incorrect_class1_as_0_ada}\")\n",
    "print(f\"Recall for class 1: {recall_class1_ada:.2f}\")\n",
    "\n",
    "# Model - 3\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, recall_score\n",
    "\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_logreg = logreg_model.predict(X_test)\n",
    "\n",
    "cm_logreg = confusion_matrix(y_test, y_pred_logreg)\n",
    "\n",
    "correct_classifications_logreg = np.trace(cm_logreg)\n",
    "incorrect_class0_as_1_logreg = cm_logreg[0, 1]\n",
    "recall_class1_logreg = recall_score(y_test, y_pred_logreg, pos_label=1)\n",
    "\n",
    "print(\"\\nLogistic Regression Results:\")\n",
    "print(f\"Correctly classified samples: {correct_classifications_logreg}\")\n",
    "print(f\"Class 0 incorrectly classified as 1: {incorrect_class0_as_1_logreg}\")\n",
    "print(f\"Recall for class 1: {recall_class1_logreg:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10592855,
     "sourceId": 90791,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
